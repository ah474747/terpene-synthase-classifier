# Code Review Guide: Key Components for AI Reviewer

## üéØ Essential Files for Review

### 1. **Main Training Pipeline** - `ts_classifier_training.py`
**Priority: CRITICAL** - This is the core implementation

**Key Classes to Review:**
- `TSGSDDataset` (lines ~50-80): Custom PyTorch dataset for feature loading
- `FocalLoss` (lines ~90-130): Multi-label focal loss implementation
- `TPSClassifier` (lines ~180-280): Complete multi-modal architecture
- `TPSModelTrainer` (lines ~290-550): Training loop with AMP and metrics

**Critical Functions:**
```python
# Focal Loss Implementation (lines 90-130)
def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
    bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
    p_t = torch.exp(-bce_loss)
    focal_weight = self.alpha * (1 - p_t) ** self.gamma
    focal_loss = focal_weight * bce_loss
    return focal_loss.mean()

# Multi-modal Fusion (lines 250-280)
def forward(self, e_plm: torch.Tensor, e_eng: torch.Tensor) -> torch.Tensor:
    plm_latent = self.plm_encoder(e_plm)  # (N, 256)
    eng_latent = self.feature_encoder(e_eng)  # (N, 256)
    fused = torch.cat([plm_latent, eng_latent], dim=1)  # (N, 512)
    logits = self.classifier(fused)  # (N, 30)
    return logits

# F1 Score Calculation Fix (lines 355-410)
def compute_metrics(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> Dict[str, float]:
    # Per-class F1 calculation then averaging - CRITICAL FIX
    for i in range(y_true_np.shape[1]):
        if y_true_np[:, i].sum() > 0:  # Only for classes with positive examples
            f1 = f1_score(y_true_np[:, i], y_pred_binary[:, i], zero_division=0)
            f1_scores.append(f1)
    macro_f1 = np.mean(f1_scores) if f1_scores else 0.0
```

### 2. **Feature Extraction Pipeline** - `ts_feature_extraction.py`
**Priority: HIGH** - Data preparation for training

**Key Functions:**
```python
# ESM2 Embedding Extraction (lines 100-150)
def extract_esm2_embeddings(self, sequences: List[str]) -> np.ndarray:
    # Mean pooling implementation - CRITICAL for protein embeddings
    attention_mask = batch_encoded['attention_mask']
    token_embeddings = outputs.last_hidden_state
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    masked_embeddings = token_embeddings * input_mask_expanded
    summed = torch.sum(masked_embeddings, 1)
    summed_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    mean_pooled = summed / summed_mask

# Engineered Feature Generation (lines 180-230)
def simulate_engineered_features(self, df: pd.DataFrame) -> np.ndarray:
    # Real categorical features + placeholder structural features
    real_features = np.concatenate([
        type_features,           # ~11 dimensions (terpene types)
        class_features,          # 2 dimensions (Class 1, Class 2)
        kingdom_features,        # ~3 dimensions (Plantae, etc.)
        normalized_products.reshape(-1, 1)  # 1 dimension
    ], axis=1)
```

### 3. **Data Consolidation Pipeline** - `marts_consolidation_pipeline.py`
**Priority: HIGH** - MARTS-DB processing

**Key Functions:**
```python
# Enzyme-Centric Consolidation (lines 80-120)
def consolidate_data(self, df: pd.DataFrame) -> pd.DataFrame:
    grouped = df.groupby('Uniprot_ID').agg({
        'Product_name': list,        # CRITICAL: Aggregate multiple products
        'Product_smiles': list,
        'Enzyme_name': 'first',     # Keep first occurrence
        'Aminoacid_sequence': 'first',
        # ... other fields
    })

# Multi-Label Target Engineering (lines 130-180)
def generate_target_vectors(self, df: pd.DataFrame) -> pd.DataFrame:
    # Map products to functional ensembles
    for product in products:
        ensemble_id = self._map_product_to_ensemble(product)
        if ensemble_id is not None:
            target_vector[ensemble_id] = 1
```

## üîç Critical Code Sections to Analyze

### 1. **Multi-Label Classification Handling**
**File:** `ts_classifier_training.py`, lines 355-410
**Why Critical:** This is where the "0.0000 F1 score" issue was identified and fixed

```python
# BEFORE (problematic):
macro_f1 = f1_score(y_true_np, y_pred_binary, average='macro', zero_division=0)

# AFTER (fixed):
for i in range(y_true_np.shape[1]):
    if y_true_np[:, i].sum() > 0:  # Only compute for classes with examples
        f1 = f1_score(y_true_np[:, i], y_pred_binary[:, i], zero_division=0)
        f1_scores.append(f1)
macro_f1 = np.mean(f1_scores) if f1_scores else 0.0
```

### 2. **Focal Loss Implementation**
**File:** `ts_classifier_training.py`, lines 90-130
**Why Critical:** Key innovation for handling class imbalance

```python
def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
    # Standard BCE
    bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
    
    # Focal weighting
    p_t = torch.exp(-bce_loss)  # Probability of true class
    focal_weight = self.alpha * (1 - p_t) ** self.gamma  # Down-weight easy examples
    
    # Apply focal weight
    focal_loss = focal_weight * bce_loss
    return focal_loss.mean()
```

### 3. **Multi-Modal Architecture**
**File:** `ts_classifier_training.py`, lines 250-280
**Why Critical:** Core innovation of the project

```python
def forward(self, e_plm: torch.Tensor, e_eng: torch.Tensor) -> torch.Tensor:
    # Process each modality separately
    plm_latent = self.plm_encoder(e_plm)      # 1280D ‚Üí 256D
    eng_latent = self.feature_encoder(e_eng)  # 64D ‚Üí 256D
    
    # Fuse modalities
    fused = torch.cat([plm_latent, eng_latent], dim=1)  # 512D
    
    # Predict classes
    logits = self.classifier(fused)  # 512D ‚Üí 30D
    return logits
```

### 4. **Training Loop with Mixed Precision**
**File:** `ts_classifier_training.py`, lines 412-470
**Why Critical:** Production-ready training implementation

```python
# Mixed Precision Training
if self.scaler is not None:
    with torch.cuda.amp.autocast():
        logits = self.model(e_plm, e_eng)
        loss = self.criterion(logits, y) / self.accumulation_steps
    
    self.scaler.scale(loss).backward()
    
    # Gradient accumulation
    if (batch_idx + 1) % self.accumulation_steps == 0:
        self.scaler.step(self.optimizer)
        self.scaler.update()
        self.optimizer.zero_grad()
```

## üß™ Test Cases to Verify

### 1. **F1 Score Calculation Test**
```python
# Test with known data
y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 0]])
y_pred = np.array([[1, 0, 0], [0, 1, 0], [1, 0, 1]])
# Expected: Should compute per-class F1 then average
```

### 2. **Data Loading Test**
```python
# Verify dataset loading
dataset = TSGSDDataset("TS-GSD_final_features.pkl")
sample = dataset[0]
assert sample[0].shape == (1280,)  # E_PLM
assert sample[1].shape == (64,)    # E_Eng  
assert sample[2].shape == (30,)    # Y
```

### 3. **Model Forward Pass Test**
```python
# Test model architecture
model = TPSClassifier()
e_plm = torch.randn(4, 1280)
e_eng = torch.randn(4, 64)
output = model(e_plm, e_eng)
assert output.shape == (4, 30)  # Batch size 4, 30 classes
```

## üîß Configuration Parameters to Review

### Training Hyperparameters
```python
LATENT_DIM = 256           # Latent space dimension
FUSED_DIM = 512            # Fusion layer dimension  
N_CLASSES = 30             # Number of functional ensembles
ACCUMULATION_STEPS = 4     # Gradient accumulation
LEARNING_RATE = 1e-4       # Adam learning rate
BATCH_SIZE = 16            # Training batch size
```

### Focal Loss Parameters
```python
alpha = 0.25               # Class weight factor
gamma = 2.0                # Focusing parameter
```

### Dataset Characteristics
```python
# From analysis:
total_samples = 1273
target_sparsity = 0.025    # 2.5% positive rate
avg_active_labels = 0.75   # Average labels per sample
```

## üö® Potential Issues to Look For

### 1. **Threshold Sensitivity**
- Current: Fixed 0.5 threshold
- Issue: Inappropriate for 2.5% positive rate
- Fix: Adaptive thresholding needed

### 2. **Memory Usage**
- Current: Batch size 16
- Issue: May be too small for GPU efficiency
- Fix: Increase with gradient accumulation

### 3. **Evaluation Metrics**
- Current: Macro F1 only
- Issue: May not capture model performance well
- Fix: Add precision@k, recall@k

### 4. **Model Complexity**
- Current: 1.2M parameters for 1,273 samples
- Issue: Potential overfitting
- Fix: Regularization tuning

## üìä Expected Performance Characteristics

### Training Behavior
- **Loss Decrease**: 0.0321 ‚Üí 0.0068 (significant learning)
- **F1 Scores**: 0.0000 (due to threshold, not model failure)
- **Convergence**: ~10 epochs with early stopping

### Model Predictions
- **Probability Range**: 0.01-0.15 (conservative, appropriate for sparse data)
- **Binary Predictions**: Mostly 0s with 0.5 threshold
- **True Behavior**: Model learns to be appropriately conservative

## üéØ Review Checklist for AI

1. **‚úÖ Architecture Soundness**: Multi-modal fusion correctly implemented
2. **‚úÖ Loss Function**: Focal Loss appropriate for imbalanced multi-label
3. **‚úÖ F1 Calculation**: Fixed implementation handles sparse data correctly
4. **‚úÖ Training Loop**: Mixed precision and gradient accumulation working
5. **‚úÖ Data Pipeline**: Proper loading and batching of features
6. **‚úÖ Model Behavior**: Conservative predictions appropriate for dataset
7. **‚ö†Ô∏è Threshold Optimization**: Needs adaptive thresholding for better F1
8. **‚ö†Ô∏è Metric Selection**: Consider alternative metrics for sparse data

## üîç Key Questions for Reviewer

1. **Is the F1 calculation fix mathematically sound?**
2. **Is the multi-modal fusion architecture appropriate?**
3. **Are the Focal Loss parameters (Œ±=0.25, Œ≥=2.0) optimal?**
4. **Is the conservative prediction behavior expected for this dataset?**
5. **What alternative evaluation metrics would be more appropriate?**
6. **How could threshold optimization be implemented?**
7. **Is the model complexity appropriate for the dataset size?**

This guide provides the reviewer with all the critical code sections, test cases, and context needed to thoroughly evaluate the implementation.
